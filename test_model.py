#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»´åº¦è¯Šæ–­è„šæœ¬
æ£€æŸ¥ audio/video backbone è¾“å‡ºç»´åº¦æ˜¯å¦ä¸ fusion æ¨¡å—åŒ¹é…
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1] / "src"))

import torch
import yaml


def diagnose_dimensions(config_path):
    """è¯Šæ–­æ¨¡å‹ç»´åº¦é…ç½®"""

    print("=" * 70)
    print("ğŸ” æ¨¡å‹ç»´åº¦è¯Šæ–­")
    print("=" * 70)

    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)

    print("\nğŸ“‹ é…ç½®æ–‡ä»¶ä¿¡æ¯:")
    print(f"  video_backbone: {cfg['model'].get('video_backbone', 'unknown')}")
    print(f"  audio_backbone: {cfg['model'].get('audio_backbone', 'unknown')}")
    print(f"  fusion_type: {cfg['model'].get('fusion_type', 'unknown')}")

    # 2. åˆ›å»ºæ¨¡å‹
    from src.avtop.models.enhanced_detector import EnhancedAVDetector

    try:
        model = EnhancedAVDetector(cfg)
        print("\nâœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return

    # 3. æ£€æŸ¥ backbone è¾“å‡ºç»´åº¦
    print("\nğŸ”§ Backbone è¾“å‡ºç»´åº¦æµ‹è¯•:")

    # æµ‹è¯•è§†é¢‘
    try:
        video_input = torch.randn(2, 16, 3, 224, 224)  # [B, T, C, H, W]
        video_feat = model.video_backbone(video_input)
        print(f"  âœ… Video Backbone:")
        print(f"     è¾“å…¥: {list(video_input.shape)}")
        print(f"     è¾“å‡º: {list(video_feat.shape)}")
        video_dim = video_feat.shape[-1]
    except Exception as e:
        print(f"  âŒ Video Backbone é”™è¯¯: {e}")
        video_dim = None

    # æµ‹è¯•éŸ³é¢‘
    try:
        audio_input = torch.randn(2, 3200)  # [B, T]
        audio_feat = model.audio_backbone(audio_input)
        print(f"  âœ… Audio Backbone:")
        print(f"     è¾“å…¥: {list(audio_input.shape)}")
        print(f"     è¾“å‡º: {list(audio_feat.shape)}")
        audio_dim = audio_feat.shape[-1]
    except Exception as e:
        print(f"  âŒ Audio Backbone é”™è¯¯: {e}")
        audio_dim = None

    # 4. æ£€æŸ¥ fusion æœŸæœ›ç»´åº¦
    print("\nğŸ”§ Fusion æ¨¡å—æœŸæœ›ç»´åº¦:")

    fusion = model.fusion

    # æ£€æŸ¥ audio_proj
    if hasattr(fusion, 'audio_proj'):
        audio_proj = fusion.audio_proj
        if hasattr(audio_proj, '0'):  # Sequential
            first_layer = audio_proj[0]
            if hasattr(first_layer, 'in_features'):
                fusion_audio_dim = first_layer.in_features
                print(f"  Audio Proj è¾“å…¥ç»´åº¦: {fusion_audio_dim}")
            else:
                fusion_audio_dim = None
        else:
            fusion_audio_dim = None
    else:
        fusion_audio_dim = None

    # æ£€æŸ¥ video_proj
    if hasattr(fusion, 'video_proj'):
        video_proj = fusion.video_proj
        if hasattr(video_proj, '0'):
            first_layer = video_proj[0]
            if hasattr(first_layer, 'in_features'):
                fusion_video_dim = first_layer.in_features
                print(f"  Video Proj è¾“å…¥ç»´åº¦: {fusion_video_dim}")
            else:
                fusion_video_dim = None
        else:
            fusion_video_dim = None
    else:
        fusion_video_dim = None

    # 5. å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 70)
    print("ğŸ“Š ç»´åº¦å¯¹æ¯”ç»“æœ:")
    print("=" * 70)

    problems = []

    if audio_dim and fusion_audio_dim:
        if audio_dim != fusion_audio_dim:
            problems.append(f"âŒ Audio ç»´åº¦ä¸åŒ¹é…: Backboneè¾“å‡º {audio_dim}, FusionæœŸæœ› {fusion_audio_dim}")
        else:
            print(f"âœ… Audio ç»´åº¦åŒ¹é…: {audio_dim}")

    if video_dim and fusion_video_dim:
        if video_dim != fusion_video_dim:
            problems.append(f"âŒ Video ç»´åº¦ä¸åŒ¹é…: Backboneè¾“å‡º {video_dim}, FusionæœŸæœ› {fusion_video_dim}")
        else:
            print(f"âœ… Video ç»´åº¦åŒ¹é…: {video_dim}")

    if problems:
        print("\nâš ï¸ å‘ç°é—®é¢˜:")
        for p in problems:
            print(f"  {p}")

        print("\nğŸ’¡ ä¿®å¤å»ºè®®:")
        if audio_dim and fusion_audio_dim and audio_dim != fusion_audio_dim:
            print(f"  æ–¹æ¡ˆ1: ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ model.audio_dim ä¸º {audio_dim}")
            print(f"  æ–¹æ¡ˆ2: ä¿®æ”¹ audio_backbone çš„è¾“å‡ºç»´åº¦ä¸º {fusion_audio_dim}")

        if video_dim and fusion_video_dim and video_dim != fusion_video_dim:
            print(f"  æ–¹æ¡ˆ1: ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„ model.video_dim ä¸º {video_dim}")
            print(f"  æ–¹æ¡ˆ2: ä¿®æ”¹ video_backbone çš„è¾“å‡ºç»´åº¦ä¸º {fusion_video_dim}")
    else:
        print("\nğŸ‰ æ‰€æœ‰ç»´åº¦åŒ¹é…ï¼Œæ¨¡å‹é…ç½®æ­£ç¡®ï¼")

    return {
        'audio_backbone_dim': audio_dim,
        'video_backbone_dim': video_dim,
        'fusion_audio_dim': fusion_audio_dim,
        'fusion_video_dim': fusion_video_dim,
        'has_problems': len(problems) > 0
    }


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1:
        config_path = sys.argv[1]
    else:
        config_path = "configs/real_binary_sota.yaml"

    print(f"ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}\n")

    result = diagnose_dimensions(config_path)

    print("\n" + "=" * 70)
    if result['has_problems']:
        print("âŒ è¯Šæ–­å®Œæˆï¼šå‘ç°ç»´åº¦ä¸åŒ¹é…é—®é¢˜")
        sys.exit(1)
    else:
        print("âœ… è¯Šæ–­å®Œæˆï¼šæ‰€æœ‰ç»´åº¦æ­£ç¡®")
        sys.exit(0)