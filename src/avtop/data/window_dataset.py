#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
window_dataset.py
çª—å£çº§åˆ«çš„éŸ³è§†é¢‘æ•°æ®é›†ï¼ˆç”¨äºè®­ç»ƒï¼‰

ç‰¹æ€§ï¼š
1. è¯»å–generate_training_dataset.pyç”Ÿæˆçš„CSV
2. æŒ‰ç…§çª—å£èŒƒå›´åŠ è½½éŸ³è§†é¢‘ç‰‡æ®µ
3. æ”¯æŒis_labeledå­—æ®µï¼ˆåŠç›‘ç£å­¦ä¹ ï¼‰
4. ç¼“å­˜æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
"""

import torch
from torch.utils.data import Dataset
import pandas as pd
import numpy as np
import soundfile as sf
import cv2
from pathlib import Path
from typing import Optional, Tuple, Callable
import warnings


class WindowDataset(Dataset):
    """
    çª—å£çº§éŸ³è§†é¢‘æ•°æ®é›†

    æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€å¯¹å¯¹é½çš„éŸ³è§†é¢‘çª—å£ï¼š
    - éŸ³é¢‘çª—å£: [audio_start_s, audio_end_s]
    - è§†é¢‘çª—å£: [video_start_frame, video_end_frame]
    """

    def __init__(
            self,
            csv_path: str,
            audio_transform: Optional[Callable] = None,
            video_transform: Optional[Callable] = None,
            target_sr: int = 16000,
            target_video_size: Tuple[int, int] = (224, 224),
            max_audio_length: float = 0.3,  # æœ€å¤§éŸ³é¢‘é•¿åº¦ï¼ˆç§’ï¼‰
            max_video_frames: int = 64,  # æœ€å¤§è§†é¢‘å¸§æ•°
            cache_mode: str = 'none'  # 'none', 'memory', 'disk'
    ):
        """
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„ï¼ˆtrain.csv / val.csv / unlabeled.csvï¼‰
            audio_transform: éŸ³é¢‘å¢å¼ºå‡½æ•°
            video_transform: è§†é¢‘å¢å¼ºå‡½æ•°
            target_sr: ç›®æ ‡é‡‡æ ·ç‡
            target_video_size: ç›®æ ‡è§†é¢‘å°ºå¯¸
            max_audio_length: æœ€å¤§éŸ³é¢‘é•¿åº¦ï¼ˆè¶…è¿‡åˆ™æˆªæ–­ï¼‰
            max_video_frames: æœ€å¤§è§†é¢‘å¸§æ•°ï¼ˆè¶…è¿‡åˆ™é‡‡æ ·ï¼‰
            cache_mode: ç¼“å­˜æ¨¡å¼
        """
        self.csv_path = Path(csv_path)
        self.audio_transform = audio_transform
        self.video_transform = video_transform

        self.target_sr = target_sr
        self.target_video_size = target_video_size
        self.max_audio_length = max_audio_length
        self.max_video_frames = max_video_frames
        self.cache_mode = cache_mode

        # åŠ è½½CSV
        self.data = pd.read_csv(csv_path)

        # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
        required_fields = [
            'video_path', 'audio_path', 'label', 'is_labeled',
            'video_start_frame', 'video_end_frame',
            'audio_start_s', 'audio_end_s'
        ]

        missing = [f for f in required_fields if f not in self.data.columns]
        if missing:
            raise ValueError(f"CSVç¼ºå°‘å­—æ®µ: {missing}")

        # è¿‡æ»¤æ— æ•ˆè®°å½•
        self.data = self.data.dropna(subset=['video_path', 'audio_path'])
        self.data = self.data.reset_index(drop=True)

        # ç¼“å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.cache = {} if cache_mode == 'memory' else None

        print(f"âœ… DatasetåŠ è½½å®Œæˆ: {len(self.data)} çª—å£å¯¹")
        print(f"   - æœ‰æ ‡ç­¾: {self.data['is_labeled'].sum()}")
        print(f"   - æ— æ ‡ç­¾: {(~self.data['is_labeled'].astype(bool)).sum()}")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """
        è¿”å›ï¼š
        {
            'audio': Tensor [C, T] - éŸ³é¢‘æ³¢å½¢
            'video': Tensor [T, C, H, W] - è§†é¢‘å¸§åºåˆ—
            'label': int - æ ‡ç­¾ (0/1/-1)
            'is_labeled': bool - æ˜¯å¦æœ‰æ ‡ç­¾
            'metadata': dict - é¢å¤–ä¿¡æ¯
        }
        """
        # æ£€æŸ¥ç¼“å­˜
        if self.cache is not None and idx in self.cache:
            return self.cache[idx]

        row = self.data.iloc[idx]

        # 1. åŠ è½½éŸ³é¢‘çª—å£
        audio = self._load_audio_window(
            audio_path=row['audio_path'],
            start_s=float(row['audio_start_s']),
            end_s=float(row['audio_end_s'])
        )

        # 2. åŠ è½½è§†é¢‘çª—å£
        video = self._load_video_window(
            video_path=row['video_path'],
            start_frame=int(row['video_start_frame']),
            end_frame=int(row['video_end_frame'])
        )

        # 3. æ•°æ®å¢å¼º
        if self.audio_transform is not None:
            audio = self.audio_transform(audio)

        if self.video_transform is not None:
            video = self.video_transform(video)

        # 4. æ„é€ æ ·æœ¬
        sample = {
            'audio': audio,
            'video': video,
            'label': int(row['label']),
            'is_labeled': bool(row['is_labeled']),
            'metadata': {
                'sample': row.get('sample', ''),
                'window_score': float(row.get('window_score', 0.0)),
                'sample_quality': float(row.get('sample_quality', 0.0))
            }
        }

        # ç¼“å­˜
        if self.cache is not None:
            self.cache[idx] = sample

        return sample

    def _load_audio_window(self, audio_path: str, start_s: float, end_s: float) -> torch.Tensor:
        """
        åŠ è½½éŸ³é¢‘çª—å£ [start_s, end_s]

        Returns:
            audio: [1, num_samples] - å•å£°é“éŸ³é¢‘
        """
        try:
            # è¯»å–å®Œæ•´éŸ³é¢‘
            y, sr = sf.read(audio_path, dtype='float32', always_2d=False)

            # è½¬å•å£°é“
            if y.ndim == 2:
                y = y.mean(axis=1)

            # é‡é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if sr != self.target_sr:
                duration = len(y) / sr
                num_samples_new = int(duration * self.target_sr)
                t_old = np.linspace(0, duration, len(y), endpoint=False)
                t_new = np.linspace(0, duration, num_samples_new, endpoint=False)
                y = np.interp(t_new, t_old, y).astype(np.float32)
                sr = self.target_sr

            # æå–çª—å£
            start_sample = int(start_s * sr)
            end_sample = int(end_s * sr)

            start_sample = max(0, min(start_sample, len(y)))
            end_sample = max(start_sample, min(end_sample, len(y)))

            window = y[start_sample:end_sample]

            # é™åˆ¶æœ€å¤§é•¿åº¦
            max_samples = int(self.max_audio_length * sr)
            if len(window) > max_samples:
                window = window[:max_samples]

            # å¡«å……åˆ°å›ºå®šé•¿åº¦ï¼ˆå¯é€‰ï¼‰
            # if len(window) < max_samples:
            #     window = np.pad(window, (0, max_samples - len(window)))

            # å½’ä¸€åŒ–
            max_val = np.abs(window).max()
            if max_val > 0:
                window = window / max_val

            # è½¬ä¸ºTensor [1, T]
            audio_tensor = torch.from_numpy(window).float().unsqueeze(0)

            return audio_tensor

        except Exception as e:
            warnings.warn(f"éŸ³é¢‘åŠ è½½å¤±è´¥ {audio_path}: {e}")
            # è¿”å›é™éŸ³
            return torch.zeros(1, int(self.max_audio_length * self.target_sr))

    def _load_video_window(self, video_path: str, start_frame: int, end_frame: int) -> torch.Tensor:
        """
        åŠ è½½è§†é¢‘çª—å£ [start_frame, end_frame]

        Returns:
            video: [T, 3, H, W] - RGBå¸§åºåˆ—
        """
        try:
            cap = cv2.VideoCapture(video_path)

            if not cap.isOpened():
                raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
            start_frame = max(0, min(start_frame, total_frames - 1))
            end_frame = max(start_frame, min(end_frame, total_frames - 1))

            # è·³åˆ°èµ·å§‹å¸§
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)

            frames = []
            for _ in range(end_frame - start_frame + 1):
                ret, frame = cap.read()
                if not ret:
                    break

                # BGR -> RGB
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # è°ƒæ•´å¤§å°
                frame = cv2.resize(frame, self.target_video_size)

                frames.append(frame)

            cap.release()

            if len(frames) == 0:
                raise RuntimeError(f"æ— æ³•è¯»å–å¸§: {video_path}")

            # å¸§é‡‡æ ·ï¼ˆå¦‚æœè¶…è¿‡æœ€å¤§æ•°é‡ï¼‰
            if len(frames) > self.max_video_frames:
                indices = np.linspace(0, len(frames) - 1, self.max_video_frames, dtype=int)
                frames = [frames[i] for i in indices]

            # è½¬ä¸ºTensor [T, H, W, C] -> [T, C, H, W]
            frames = np.stack(frames, axis=0)  # [T, H, W, 3]
            frames = torch.from_numpy(frames).float().permute(0, 3, 1, 2)  # [T, 3, H, W]

            # å½’ä¸€åŒ–åˆ°[0, 1]
            frames = frames / 255.0

            return frames

        except Exception as e:
            warnings.warn(f"è§†é¢‘åŠ è½½å¤±è´¥ {video_path}: {e}")
            # è¿”å›é»‘å±
            return torch.zeros(
                self.max_video_frames, 3,
                self.target_video_size[0],
                self.target_video_size[1]
            )

    def get_label_distribution(self):
        """è·å–æ ‡ç­¾åˆ†å¸ƒï¼ˆä»…æœ‰æ ‡ç­¾æ ·æœ¬ï¼‰"""
        labeled = self.data[self.data['is_labeled'] == 1]
        if len(labeled) == 0:
            return {}
        return labeled['label'].value_counts().to_dict()

    def get_quality_stats(self):
        """è·å–è´¨é‡ç»Ÿè®¡"""
        if 'sample_quality' not in self.data.columns:
            return None

        return {
            'mean': self.data['sample_quality'].mean(),
            'median': self.data['sample_quality'].median(),
            'min': self.data['sample_quality'].min(),
            'max': self.data['sample_quality'].max()
        }


def collate_fn(batch):
    """
    è‡ªå®šä¹‰collateå‡½æ•°ï¼ˆå¤„ç†å˜é•¿åºåˆ—ï¼‰

    ç­–ç•¥ï¼š
    - éŸ³é¢‘ï¼šå¡«å……åˆ°batchå†…æœ€å¤§é•¿åº¦
    - è§†é¢‘ï¼šå¡«å……åˆ°batchå†…æœ€å¤§å¸§æ•°

    ğŸ”§ ä¿®å¤ï¼šå°†éŸ³é¢‘ä» [B, 1, T] squeeze ä¸º [B, T]
    """
    # åˆ†ç¦»å„ä¸ªå­—æ®µ
    audios = [item['audio'] for item in batch]
    videos = [item['video'] for item in batch]
    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
    is_labeled = torch.tensor([item['is_labeled'] for item in batch], dtype=torch.bool)

    # å¡«å……éŸ³é¢‘
    max_audio_len = max(a.shape[1] for a in audios)
    audios_padded = []
    for audio in audios:
        if audio.shape[1] < max_audio_len:
            padding = torch.zeros(1, max_audio_len - audio.shape[1])
            audio = torch.cat([audio, padding], dim=1)
        audios_padded.append(audio)
    audios = torch.stack(audios_padded, dim=0)  # [B, 1, T]

    # === ğŸ”§ å…³é”®ä¿®å¤ï¼šå»é™¤é€šé“ç»´åº¦ ===
    # ä» [B, 1, T] -> [B, T]
    audios = audios.squeeze(1)
    # ================================

    # å¡«å……è§†é¢‘
    max_video_frames = max(v.shape[0] for v in videos)
    videos_padded = []
    for video in videos:
        if video.shape[0] < max_video_frames:
            padding = torch.zeros(
                max_video_frames - video.shape[0],
                video.shape[1], video.shape[2], video.shape[3]
            )
            video = torch.cat([video, padding], dim=0)
        videos_padded.append(video)
    videos = torch.stack(videos_padded, dim=0)  # [B, T, C, H, W]

    return {
        'audio': audios,  # ç°åœ¨æ˜¯ [B, T] âœ…
        'video': videos,  # [B, T, C, H, W]
        'label': labels,  # [B]
        'is_labeled': is_labeled  # [B]
    }


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    from torch.utils.data import DataLoader

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = WindowDataset(
        csv_path="data/train.csv",
        target_sr=16000,
        target_video_size=(224, 224),
        max_audio_length=0.3,
        max_video_frames=64
    )

    val_dataset = WindowDataset(
        csv_path="data/val.csv",
        target_sr=16000,
        target_video_size=(224, 224)
    )

    print(f"\nè®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")

    # æŸ¥çœ‹æ ·æœ¬
    sample = train_dataset[0]
    print(f"\næ ·æœ¬å½¢çŠ¶:")
    print(f"  éŸ³é¢‘: {sample['audio'].shape}")
    print(f"  è§†é¢‘: {sample['video'].shape}")
    print(f"  æ ‡ç­¾: {sample['label']}")
    print(f"  æœ‰æ ‡ç­¾: {sample['is_labeled']}")

    # åˆ›å»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=4,
        shuffle=True,
        num_workers=2,
        collate_fn=collate_fn
    )

    # æµ‹è¯•ä¸€ä¸ªbatch
    batch = next(iter(train_loader))
    print(f"\nBatchå½¢çŠ¶:")
    print(f"  éŸ³é¢‘: {batch['audio'].shape}")  # åº”è¯¥æ˜¯ [4, T] âœ…
    print(f"  è§†é¢‘: {batch['video'].shape}")  # [4, T, 3, 224, 224]
    print(f"  æ ‡ç­¾: {batch['label'].shape}")  # [4]
    print(f"  æœ‰æ ‡ç­¾: {batch['is_labeled'].shape}")  # [4]

    # === é¢å¤–éªŒè¯ï¼šç¡®ä¿éŸ³é¢‘å½¢çŠ¶æ­£ç¡® ===
    assert batch['audio'].dim() == 2, f"éŸ³é¢‘åº”è¯¥æ˜¯2ç»´ [B, T]ï¼Œå®é™…æ˜¯ {batch['audio'].dim()}ç»´"
    print(f"\nâœ… éŸ³é¢‘å½¢çŠ¶éªŒè¯é€šè¿‡: {batch['audio'].shape} (æœŸæœ› [B, T])")

    print("\nâœ… Datasetæµ‹è¯•é€šè¿‡ï¼")