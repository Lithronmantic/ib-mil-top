# src/avtop/eval/enhanced_metrics.py
"""
æ”¹è¿›çš„è¯„ä¼°æŒ‡æ ‡æ¨¡å—
é‡ç‚¹ï¼šå°‘æ•°ç±»AUPRCã€MCCã€å¸¦baselineå¯¹æ¯”
"""
import numpy as np
from sklearn import metrics
from typing import Dict, Tuple


class ImbalancedMetricsCalculator:
    """ä¸“é—¨é’ˆå¯¹ä¸å¹³è¡¡æ•°æ®çš„æŒ‡æ ‡è®¡ç®—å™¨"""

    def __init__(self, minority_class: int = 1):
        """
        Args:
            minority_class: å°‘æ•°ç±»/å…³æ³¨ç±»çš„æ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯å¼‚å¸¸ç±»=1ï¼‰
        """
        self.minority_class = minority_class

    def compute_all_metrics(self, y_true: np.ndarray, y_probs: np.ndarray,
                            threshold: float = 0.5) -> Dict[str, float]:
        """
        è®¡ç®—æ‰€æœ‰å…³é”®æŒ‡æ ‡

        Args:
            y_true: çœŸå®æ ‡ç­¾ (N,)
            y_probs: é¢„æµ‹æ¦‚ç‡ (N,) - æ­£ç±»çš„æ¦‚ç‡
            threshold: åˆ†ç±»é˜ˆå€¼

        Returns:
            åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        """
        y_pred = (y_probs >= threshold).astype(int)

        # 1. åŸºç¡€ç»Ÿè®¡
        n_total = len(y_true)
        n_pos = np.sum(y_true == self.minority_class)
        n_neg = n_total - n_pos
        pos_ratio = n_pos / n_total if n_total > 0 else 0

        # 2. â­ å°‘æ•°ç±»AUPRCï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
        if n_pos > 0 and n_neg > 0:
            precision, recall, _ = metrics.precision_recall_curve(
                y_true, y_probs, pos_label=self.minority_class
            )
            auprc_minority = metrics.auc(recall, precision)

            # è®¡ç®—baselineï¼ˆéšæœºåˆ†ç±»å™¨çš„æœŸæœ›APï¼‰
            auprc_baseline = pos_ratio
            auprc_gain = auprc_minority - auprc_baseline
        else:
            auprc_minority = 0.0
            auprc_baseline = 0.0
            auprc_gain = 0.0

        # 3. â­ MCCï¼ˆMatthewsç›¸å…³ç³»æ•°ï¼Œå¯¹ä¸å¹³è¡¡é²æ£’ï¼‰
        try:
            mcc = metrics.matthews_corrcoef(y_true, y_pred)
        except:
            mcc = 0.0

        # 4. ROC-AUCï¼ˆå‚è€ƒæŒ‡æ ‡ï¼‰
        if n_pos > 0 and n_neg > 0:
            roc_auc = metrics.roc_auc_score(y_true, y_probs)
        else:
            roc_auc = 0.5

        # 5. æ··æ·†çŸ©é˜µç›¸å…³
        cm = metrics.confusion_matrix(y_true, y_pred)
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
        else:
            tn = fp = fn = tp = 0

        # 6. å°‘æ•°ç±»çš„ Precision, Recall, F1
        if tp + fp > 0:
            precision_minority = tp / (tp + fp)
        else:
            precision_minority = 0.0

        if tp + fn > 0:
            recall_minority = tp / (tp + fn)
        else:
            recall_minority = 0.0

        if precision_minority + recall_minority > 0:
            f1_minority = 2 * precision_minority * recall_minority / (precision_minority + recall_minority)
        else:
            f1_minority = 0.0

        # 7. ç‰¹å¼‚æ€§ï¼ˆå¯¹å¤šæ•°ç±»çš„å¬å›ç‡ï¼‰
        if tn + fp > 0:
            specificity = tn / (tn + fp)
        else:
            specificity = 0.0

        # 8. G-Meanï¼ˆå‡ ä½•å¹³å‡ï¼Œå¹³è¡¡ä¸¤ç±»ï¼‰
        gmean = np.sqrt(recall_minority * specificity)

        return {
            # â­ ä¸»è¦æŒ‡æ ‡
            'auprc_minority': float(auprc_minority),
            'auprc_baseline': float(auprc_baseline),
            'auprc_gain': float(auprc_gain),
            'mcc': float(mcc),

            # å°‘æ•°ç±»æ€§èƒ½
            'precision_minority': float(precision_minority),
            'recall_minority': float(recall_minority),
            'f1_minority': float(f1_minority),

            # å¹³è¡¡æŒ‡æ ‡
            'gmean': float(gmean),
            'specificity': float(specificity),

            # å‚è€ƒæŒ‡æ ‡
            'roc_auc': float(roc_auc),

            # ç»Ÿè®¡ä¿¡æ¯
            'pos_ratio': float(pos_ratio),
            'n_pos': int(n_pos),
            'n_neg': int(n_neg),
            'threshold': float(threshold),

            # æ··æ·†çŸ©é˜µ
            'tp': int(tp),
            'fp': int(fp),
            'tn': int(tn),
            'fn': int(fn)
        }

    def find_best_threshold(self, y_true: np.ndarray, y_probs: np.ndarray,
                            metric: str = 'mcc') -> Tuple[float, float]:
        """
        åœ¨éªŒè¯é›†ä¸Šå¯»æ‰¾æœ€ä¼˜é˜ˆå€¼

        Args:
            y_true: çœŸå®æ ‡ç­¾
            y_probs: é¢„æµ‹æ¦‚ç‡
            metric: ä¼˜åŒ–ç›®æ ‡ ('mcc', 'f1', 'gmean', 'youden')

        Returns:
            (best_threshold, best_metric_value)
        """
        thresholds = np.linspace(0.01, 0.99, 99)
        best_value = -1.0
        best_thresh = 0.5

        for th in thresholds:
            y_pred = (y_probs >= th).astype(int)

            if metric == 'mcc':
                try:
                    value = metrics.matthews_corrcoef(y_true, y_pred)
                except:
                    value = 0.0
            elif metric == 'f1':
                value = metrics.f1_score(y_true, y_pred, pos_label=self.minority_class, zero_division=0)
            elif metric == 'gmean':
                cm = metrics.confusion_matrix(y_true, y_pred)
                if cm.shape == (2, 2):
                    tn, fp, fn, tp = cm.ravel()
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                    spec = tn / (tn + fp) if (tn + fp) > 0 else 0
                    value = np.sqrt(recall * spec)
                else:
                    value = 0.0
            elif metric == 'youden':
                # Youden's J statistic = sensitivity + specificity - 1
                cm = metrics.confusion_matrix(y_true, y_pred)
                if cm.shape == (2, 2):
                    tn, fp, fn, tp = cm.ravel()
                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                    spec = tn / (tn + fp) if (tn + fp) > 0 else 0
                    value = recall + spec - 1
                else:
                    value = 0.0
            else:
                raise ValueError(f"Unknown metric: {metric}")

            if value > best_value:
                best_value = value
                best_thresh = th

        return best_thresh, best_value

    def print_report(self, metrics_dict: Dict[str, float], name: str = "Validation"):
        """æ‰“å°æ ¼å¼åŒ–çš„æŒ‡æ ‡æŠ¥å‘Š"""
        print(f"\n{'=' * 60}")
        print(f"{name} Metrics Report")
        print(f"{'=' * 60}")

        # æ•°æ®åˆ†å¸ƒ
        print(f"\nğŸ“Š Data Distribution:")
        print(f"  Positive: {metrics_dict['n_pos']} ({metrics_dict['pos_ratio']:.2%})")
        print(f"  Negative: {metrics_dict['n_neg']} ({1 - metrics_dict['pos_ratio']:.2%})")
        print(f"  Threshold: {metrics_dict['threshold']:.3f}")

        # â­ ä¸»è¦æŒ‡æ ‡
        print(f"\nâ­ Primary Metrics (Imbalance-Robust):")
        print(f"  AUPRC (Minority):  {metrics_dict['auprc_minority']:.4f}")
        print(f"  AUPRC Baseline:    {metrics_dict['auprc_baseline']:.4f}")
        print(
            f"  AUPRC Gain:        {metrics_dict['auprc_gain']:.4f} ({'â†‘' if metrics_dict['auprc_gain'] > 0 else 'â†“'})")
        print(f"  MCC:               {metrics_dict['mcc']:.4f}")

        # å°‘æ•°ç±»æ€§èƒ½
        print(f"\nğŸ¯ Minority Class Performance:")
        print(f"  Precision: {metrics_dict['precision_minority']:.4f}")
        print(f"  Recall:    {metrics_dict['recall_minority']:.4f}")
        print(f"  F1-Score:  {metrics_dict['f1_minority']:.4f}")

        # å¹³è¡¡æŒ‡æ ‡
        print(f"\nâš–ï¸  Balanced Metrics:")
        print(f"  G-Mean:      {metrics_dict['gmean']:.4f}")
        print(f"  Specificity: {metrics_dict['specificity']:.4f}")

        # å‚è€ƒæŒ‡æ ‡
        print(f"\nğŸ“ˆ Reference:")
        print(f"  ROC-AUC: {metrics_dict['roc_auc']:.4f}")

        # æ··æ·†çŸ©é˜µ
        print(f"\nğŸ“‹ Confusion Matrix:")
        print(f"              Predicted")
        print(f"              Neg    Pos")
        print(f"  Actual Neg  {metrics_dict['tn']:4d}   {metrics_dict['fp']:4d}")
        print(f"         Pos  {metrics_dict['fn']:4d}   {metrics_dict['tp']:4d}")

        print(f"{'=' * 60}\n")


def validate_model(model, dataloader, device, minority_class: int = 1):
    """
    æ¨¡å‹éªŒè¯å‡½æ•°ï¼ˆä½¿ç”¨æ”¹è¿›çš„æŒ‡æ ‡ï¼‰

    Returns:
        metrics_dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸
        probs: é¢„æµ‹æ¦‚ç‡ (N,)
        labels: çœŸå®æ ‡ç­¾ (N,)
    """
    import torch

    model.eval()
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            video = batch["video"].to(device)
            audio = batch["audio"].to(device)
            labels = batch["label_idx"]

            # å‰å‘ä¼ æ’­
            out = model(video, audio)

            # è·å–æ­£ç±»æ¦‚ç‡
            if "clip_logits" in out:
                logits = out["clip_logits"]
                probs = torch.softmax(logits, dim=-1)[:, minority_class]
            else:
                # å¦‚æœæ˜¯å•ä¸ªlogit
                probs = torch.sigmoid(out)

            all_probs.append(probs.cpu().numpy())
            all_labels.append(labels.numpy())

    all_probs = np.concatenate(all_probs)
    all_labels = np.concatenate(all_labels)

    # è®¡ç®—æŒ‡æ ‡
    calc = ImbalancedMetricsCalculator(minority_class=minority_class)

    # å…ˆæ‰¾æœ€ä¼˜é˜ˆå€¼ï¼ˆåŸºäºMCCï¼‰
    best_thresh, best_mcc = calc.find_best_threshold(all_labels, all_probs, metric='mcc')

    # ç”¨æœ€ä¼˜é˜ˆå€¼è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    metrics_dict = calc.compute_all_metrics(all_labels, all_probs, threshold=best_thresh)
    metrics_dict['best_threshold'] = best_thresh

    return metrics_dict, all_probs, all_labels