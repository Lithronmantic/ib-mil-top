#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
gram_contrastive.py - ä¿®å¤ç‰ˆæœ¬
å…³é”®ä¿®å¤ï¼šé˜²æ­¢å¯¹æ¯”æŸå¤±å˜ä¸ºè´Ÿæ•°
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional


class GRAMContrastiveLoss(nn.Module):
    """GRAMå¯¹æ¯”å­¦ä¹ æŸå¤± - ä¿®å¤ç‰ˆ"""

    def __init__(self, temperature: float = 0.07, use_volume_metric: bool = False):  # ğŸ”§ é»˜è®¤å…³é—­GRAM
        super().__init__()
        self.temperature = max(temperature, 0.05)  # ğŸ”§ æ¸©åº¦ä¸‹é™
        self.use_volume_metric = use_volume_metric

    def forward(self, z_a: torch.Tensor, z_v: torch.Tensor) -> torch.Tensor:
        """
        Args:
            z_a: [B, D] - éŸ³é¢‘åµŒå…¥ï¼ˆL2å½’ä¸€åŒ–ï¼‰
            z_v: [B, D] - è§†é¢‘åµŒå…¥ï¼ˆL2å½’ä¸€åŒ–ï¼‰
        Returns:
            loss: æ ‡é‡
        """
        B = z_a.shape[0]

        # ç¡®ä¿L2å½’ä¸€åŒ–
        z_a = F.normalize(z_a, p=2, dim=-1)
        z_v = F.normalize(z_v, p=2, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        sim_matrix = torch.matmul(z_a, z_v.t()) / self.temperature  # [B, B]

        if self.use_volume_metric:
            loss = self._gram_volume_loss(sim_matrix)
        else:
            # ğŸ”§ ä½¿ç”¨æ›´ç¨³å®šçš„InfoNCE
            loss = self._infonce_loss(sim_matrix)

        return loss

    def _infonce_loss(self, sim_matrix: torch.Tensor) -> torch.Tensor:
        """æ ‡å‡†InfoNCEå¯¹æ¯”æŸå¤±"""
        B = sim_matrix.shape[0]
        labels = torch.arange(B, device=sim_matrix.device)

        # Audio -> Video
        loss_a2v = F.cross_entropy(sim_matrix, labels)

        # Video -> Audio
        loss_v2a = F.cross_entropy(sim_matrix.t(), labels)

        # å¯¹ç§°æŸå¤±
        loss = (loss_a2v + loss_v2a) / 2.0

        return loss

    def _gram_volume_loss(self, sim_matrix: torch.Tensor) -> torch.Tensor:
        """GRAMä½“ç§¯å¯¹æ¯”æŸå¤± - ä¿®å¤ç‰ˆ"""
        B = sim_matrix.shape[0]

        # æ­£æ ·æœ¬åˆ†æ•°
        pos_scores = torch.diag(sim_matrix)  # [B]

        # è´Ÿæ ·æœ¬åˆ†æ•°çŸ©é˜µï¼ˆå»é™¤å¯¹è§’çº¿ï¼‰
        mask = torch.eye(B, device=sim_matrix.device).bool()
        neg_scores = sim_matrix.masked_fill(mask, float('-inf'))  # [B, B]

        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è´Ÿæ ·æœ¬"ä½“ç§¯"ï¼ˆlog-sum-expï¼‰
        neg_volume_a2v = torch.logsumexp(neg_scores, dim=1)  # [B]
        neg_volume_v2a = torch.logsumexp(neg_scores.t(), dim=1)  # [B]

        # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æŸå¤±éè´Ÿ
        # loss = -log(exp(pos) / (exp(pos) + exp(neg_volume)))
        #      = -pos + log(exp(pos) + exp(neg_volume))
        #      = -pos + logsumexp([pos, neg_volume])

        loss_a2v = -pos_scores + torch.logsumexp(
            torch.stack([pos_scores, neg_volume_a2v], dim=0), dim=0
        )
        loss_v2a = -pos_scores + torch.logsumexp(
            torch.stack([pos_scores, neg_volume_v2a], dim=0), dim=0
        )

        # å¹³å‡
        loss = (loss_a2v.mean() + loss_v2a.mean()) / 2.0

        # ğŸ”§ å®‰å…¨æ£€æŸ¥
        if torch.isnan(loss) or torch.isinf(loss) or loss < 0:
            # é™çº§ä¸ºInfoNCE
            return self._infonce_loss(sim_matrix / self.temperature)

        return loss


class BidirectionalKDLoss(nn.Module):
    """åŒå‘çŸ¥è¯†è’¸é¦æŸå¤±"""

    def __init__(self, temperature: float = 4.0):
        super().__init__()
        self.temperature = temperature
        self.kl_div = nn.KLDivLoss(reduction='batchmean')

    def forward(
            self,
            logits_fusion: torch.Tensor,
            logits_audio: Optional[torch.Tensor],
            logits_video: Optional[torch.Tensor]
    ) -> torch.Tensor:
        """
        Args:
            logits_fusion: [B, C] - èåˆæ¨¡å‹è¾“å‡º
            logits_audio: [B, C] - éŸ³é¢‘å•æ¨¡æ€è¾“å‡ºï¼ˆå¯é€‰ï¼‰
            logits_video: [B, C] - è§†é¢‘å•æ¨¡æ€è¾“å‡ºï¼ˆå¯é€‰ï¼‰
        Returns:
            loss: æ ‡é‡
        """
        if logits_audio is None and logits_video is None:
            return torch.tensor(0.0, device=logits_fusion.device)

        T = self.temperature
        loss = 0.0
        count = 0

        # è½¯åŒ–æ¦‚ç‡åˆ†å¸ƒ
        p_fusion = F.log_softmax(logits_fusion / T, dim=-1)

        # 1. Audio â†’ Fusion
        if logits_audio is not None:
            q_audio = F.softmax(logits_audio / T, dim=-1)
            loss += self.kl_div(p_fusion, q_audio) * (T * T)
            count += 1

        # 2. Video â†’ Fusion
        if logits_video is not None:
            q_video = F.softmax(logits_video / T, dim=-1)
            loss += self.kl_div(p_fusion, q_video) * (T * T)
            count += 1

        # 3. Fusion â†’ Audio
        if logits_audio is not None:
            p_audio = F.log_softmax(logits_audio / T, dim=-1)
            q_fusion_for_audio = F.softmax(logits_fusion / T, dim=-1)
            loss += self.kl_div(p_audio, q_fusion_for_audio) * (T * T)
            count += 1

        # 4. Fusion â†’ Video
        if logits_video is not None:
            p_video = F.log_softmax(logits_video / T, dim=-1)
            q_fusion_for_video = F.softmax(logits_fusion / T, dim=-1)
            loss += self.kl_div(p_video, q_fusion_for_video) * (T * T)
            count += 1

        # å¹³å‡
        if count > 0:
            loss = loss / count

        return loss


class ConsistencyLoss(nn.Module):
    """è·¨æ¨¡æ€ä¸€è‡´æ€§æŸå¤±"""

    def __init__(self, consistency_type: str = 'cosine'):
        super().__init__()
        self.consistency_type = consistency_type

    def forward(
            self,
            z_a: torch.Tensor,
            z_v: torch.Tensor,
            audio_feat: Optional[torch.Tensor] = None,
            video_feat: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            z_a: [B, D] - éŸ³é¢‘å…¨å±€åµŒå…¥
            z_v: [B, D] - è§†é¢‘å…¨å±€åµŒå…¥
            audio_feat: [B, T, D] - éŸ³é¢‘ç‰¹å¾åºåˆ—ï¼ˆå¯é€‰ï¼‰
            video_feat: [B, T, D] - è§†é¢‘ç‰¹å¾åºåˆ—ï¼ˆå¯é€‰ï¼‰
        Returns:
            loss: æ ‡é‡
        """
        loss = 0.0
        count = 0

        # 1. è·¨æ¨¡æ€åµŒå…¥ä¸€è‡´æ€§
        if self.consistency_type == 'cosine':
            cos_sim = F.cosine_similarity(z_a, z_v, dim=-1)
            loss += (1.0 - cos_sim).mean()
        elif self.consistency_type == 'mse':
            loss += F.mse_loss(z_a, z_v)
        else:
            loss += F.l1_loss(z_a, z_v)
        count += 1

        # 2. æ—¶åºä¸€è‡´æ€§ï¼ˆç›¸é‚»å¸§ï¼‰
        if audio_feat is not None:
            temporal_loss_a = self._temporal_consistency(audio_feat)
            loss += temporal_loss_a
            count += 1

        if video_feat is not None:
            temporal_loss_v = self._temporal_consistency(video_feat)
            loss += temporal_loss_v
            count += 1

        # å¹³å‡
        if count > 0:
            loss = loss / count

        return loss

    def _temporal_consistency(self, feat: torch.Tensor) -> torch.Tensor:
        """æ—¶åºä¸€è‡´æ€§ï¼šç›¸é‚»å¸§çš„ç‰¹å¾åº”è¯¥ç›¸ä¼¼"""
        if feat.shape[1] <= 1:
            return torch.tensor(0.0, device=feat.device)

        # è®¡ç®—ç›¸é‚»å¸§ä¹‹é—´çš„å·®å¼‚
        diff = feat[:, 1:, :] - feat[:, :-1, :]  # [B, T-1, D]

        # L2èŒƒæ•°ï¼ˆå¸Œæœ›å°ï¼‰
        loss = torch.norm(diff, p=2, dim=-1).mean()

        return loss


class CompleteLossFunction(nn.Module):
    """å®Œæ•´çš„å¤šä»»åŠ¡æŸå¤±å‡½æ•° - ä¿®å¤ç‰ˆ"""

    def __init__(
            self,
            num_classes: int = 2,
            lambda_contrastive: float = 0.3,
            lambda_kd: float = 0.2,
            lambda_consistency: float = 0.1,
            temperature: float = 0.07,
            kd_temperature: float = 4.0,
            use_focal_loss: bool = False,
            focal_alpha: float = 0.25,
            focal_gamma: float = 2.0
    ):
        super().__init__()

        self.num_classes = num_classes
        self.lambda_contrastive = lambda_contrastive
        self.lambda_kd = lambda_kd
        self.lambda_consistency = lambda_consistency

        # åˆ†ç±»æŸå¤±
        if use_focal_loss:
            self.cls_criterion = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)
        else:
            self.cls_criterion = nn.CrossEntropyLoss(reduction='mean')

        # ğŸ”§ å¯¹æ¯”å­¦ä¹ ï¼šé»˜è®¤ä½¿ç”¨ç¨³å®šçš„InfoNCE
        self.contrastive_criterion = GRAMContrastiveLoss(
            temperature=temperature,
            use_volume_metric=False  # ğŸ”§ å…³é—­GRAM
        )

        # åŒå‘KD
        self.kd_criterion = BidirectionalKDLoss(temperature=kd_temperature)

        # ä¸€è‡´æ€§æŸå¤±
        self.consistency_criterion = ConsistencyLoss(consistency_type='cosine')

        print("[CompleteLossFunction] åˆå§‹åŒ–å®Œæˆ")
        print(f"  - å¯¹æ¯”å­¦ä¹ æƒé‡: {lambda_contrastive}")
        print(f"  - KDæƒé‡: {lambda_kd}")
        print(f"  - ä¸€è‡´æ€§æƒé‡: {lambda_consistency}")
        print(f"  - æ¸©åº¦å‚æ•°: {temperature}")

    def forward(
            self,
            outputs: Dict[str, torch.Tensor],
            labels: torch.Tensor,
            is_labeled: torch.Tensor
    ) -> Dict[str, torch.Tensor]:
        """
        Args:
            outputs: æ¨¡å‹è¾“å‡ºå­—å…¸
            labels: [B] - æ ‡ç­¾
            is_labeled: [B] - æ˜¯å¦æœ‰æ ‡ç­¾çš„mask
        Returns:
            loss_dict: åŒ…å«æ‰€æœ‰æŸå¤±çš„å­—å…¸
        """
        # æå–è¾“å‡º
        clip_logits = outputs['clip_logits']
        z_v = outputs.get('z_v')
        z_a = outputs.get('z_a')
        video_logits = outputs.get('video_logits')
        audio_logits = outputs.get('audio_logits')
        video_feat = outputs.get('video_feat')
        audio_feat = outputs.get('audio_feat')

        device = clip_logits.device

        # 1. åˆ†ç±»æŸå¤±
        if is_labeled.sum() > 0:
            labeled_mask = is_labeled.bool()

            loss_cls_fusion = self.cls_criterion(
                clip_logits[labeled_mask],
                labels[labeled_mask]
            )

            loss_cls_aux = 0.0
            if video_logits is not None:
                loss_cls_aux += self.cls_criterion(
                    video_logits[labeled_mask],
                    labels[labeled_mask]
                )
            if audio_logits is not None:
                loss_cls_aux += self.cls_criterion(
                    audio_logits[labeled_mask],
                    labels[labeled_mask]
                )

            classification_loss = loss_cls_fusion + 0.5 * loss_cls_aux
        else:
            classification_loss = torch.tensor(0.0, device=device)

        # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±
        if z_v is not None and z_a is not None:
            contrastive_loss = self.contrastive_criterion(z_a, z_v)

            # ğŸ”§ å®‰å…¨æ£€æŸ¥
            if torch.isnan(contrastive_loss) or torch.isinf(contrastive_loss):
                print("âš ï¸ å¯¹æ¯”æŸå¤±å¼‚å¸¸ï¼Œè®¾ä¸º0")
                contrastive_loss = torch.tensor(0.0, device=device)
            elif contrastive_loss < 0:
                print(f"âš ï¸ å¯¹æ¯”æŸå¤±ä¸ºè´Ÿæ•°: {contrastive_loss.item():.4f}ï¼Œè®¾ä¸º0")
                contrastive_loss = torch.tensor(0.0, device=device)
        else:
            contrastive_loss = torch.tensor(0.0, device=device)

        # 3. KDæŸå¤±
        if (video_logits is not None or audio_logits is not None):
            kd_loss = self.kd_criterion(clip_logits, audio_logits, video_logits)
        else:
            kd_loss = torch.tensor(0.0, device=device)

        # 4. ä¸€è‡´æ€§æŸå¤±
        if z_v is not None and z_a is not None:
            consistency_loss = self.consistency_criterion(
                z_a, z_v, audio_feat, video_feat
            )
        else:
            consistency_loss = torch.tensor(0.0, device=device)

        # 5. æ€»æŸå¤±
        total_loss = (
                classification_loss +
                self.lambda_contrastive * contrastive_loss +
                self.lambda_kd * kd_loss +
                self.lambda_consistency * consistency_loss
        )

        # ğŸ”§ æœ€ç»ˆå®‰å…¨æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print("âš ï¸ æ€»æŸå¤±å¼‚å¸¸ï¼Œé™çº§ä¸ºåˆ†ç±»æŸå¤±")
            total_loss = classification_loss

        return {
            'total_loss': total_loss,
            'classification_loss': classification_loss,
            'contrastive_loss': contrastive_loss,
            'kd_loss': kd_loss,
            'consistency_loss': consistency_loss
        }


class FocalLoss(nn.Module):
    """Focal Lossï¼ˆç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰"""

    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, reduction: str = 'mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        ce_loss = F.cross_entropy(logits, targets, reduction='none')
        p_t = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss

        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


if __name__ == "__main__":
    # æµ‹è¯•
    B, C, D = 8, 2, 128

    outputs = {
        'clip_logits': torch.randn(B, C),
        'z_v': F.normalize(torch.randn(B, D), p=2, dim=-1),
        'z_a': F.normalize(torch.randn(B, D), p=2, dim=-1),
        'video_logits': torch.randn(B, C),
        'audio_logits': torch.randn(B, C),
    }

    labels = torch.randint(0, C, (B,))
    is_labeled = torch.tensor([True] * B)

    criterion = CompleteLossFunction()
    loss_dict = criterion(outputs, labels, is_labeled)

    print("\næŸå¤±:")
    for k, v in loss_dict.items():
        print(f"  {k}: {v.item():.4f}")

    print("\nâœ… æµ‹è¯•é€šè¿‡")