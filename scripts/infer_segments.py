#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•´æ®µè§†é¢‘ç¼ºé™·æ£€æµ‹ï¼ˆæ»‘çª— + å¸¦æ ‡ç­¾è§†é¢‘æ¸²æŸ“ + éŸ³é¢‘å¯è§†åŒ–ï¼‰
è¾“å‡ºï¼š
  1) segments.csv        : æ¯ä¸ªæ—¶é—´çª—å£çš„ç¼ºé™·æ¦‚ç‡ä¸é¢„æµ‹
  2) merged_segments.csv : è¿ç»­çª—å£åˆå¹¶åçš„ç¼ºé™·/æ­£å¸¸åŒºæ®µï¼ˆç§’ï¼‰
  3) timeline.png        : æ¦‚ç‡æ—¶é—´è½´+é˜ˆå€¼+åˆå¹¶åŒºæ®µ
  4) summary.txt         : è§†é¢‘çº§ç»“è®ºä¸å…³é”®ç»Ÿè®¡
  5) annotated.mp4       : å åŠ æ ‡ç­¾ä¸éŸ³é¢‘å¯è§†åŒ–çš„æ¼”ç¤ºè§†é¢‘   â† æ–°å¢

ç”¨æ³•ç¤ºä¾‹ï¼š
python scripts/infer_segments.py ^
  --video demo/xxx.mp4 ^
  --checkpoint outputs/sota_training/checkpoints/best_model.pth ^
  --config configs/real_binary_sota.yaml ^
  --out_dir outputs/infer_xxx ^
  --win_s 0.30 --hop_s 0.10 --v_frames 16 --v_stride 1 ^
  --thr 0.50 ^
  --render_video --show_audio
"""
import os
import sys
import math
import csv
import yaml
import argparse
from pathlib import Path
from typing import Tuple, List, Optional

import numpy as np
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import cv2

# é¡¹ç›®å†…å¯¼å…¥
PROJ_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(PROJ_ROOT))
from src.avtop.models.enhanced_detector import EnhancedAVDetector

IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)
IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)

# ---------------- åŸºç¡€ I/O ----------------

def load_audio(audio_path: str, target_sr: int) -> np.ndarray:
    """è¯»éŸ³é¢‘ä¸ºå•é€šé“ float32ï¼Œé‡‡æ ·ç‡ target_srã€‚ä¼˜å…ˆ librosaï¼›å¦åˆ™ soundfile+çº¿æ€§é‡é‡‡æ ·ã€‚"""
    try:
        import librosa
        y, _ = librosa.load(audio_path, sr=target_sr, mono=True)
        y = y.astype(np.float32)
    except Exception:
        import soundfile as sf
        y, sr0 = sf.read(audio_path)
        if y.ndim == 2: y = y.mean(axis=1)
        if sr0 != target_sr:
            x = np.linspace(0, len(y), int(len(y) * target_sr / sr0), endpoint=False)
            y = np.interp(x, np.arange(len(y)), y)
        y = y.astype(np.float32)
    mx = float(np.max(np.abs(y))) if len(y) else 1.0
    return y / max(mx, 1e-6)

def extract_audio_from_video(video_path: str, target_sr: int) -> np.ndarray:
    """ä»è§†é¢‘æå–éŸ³é¢‘ï¼Œè‹¥å¤±è´¥æé†’æä¾› --audioã€‚"""
    try:
        import librosa
        y, _ = librosa.load(video_path, sr=target_sr, mono=True)
        y = y.astype(np.float32)
        mx = float(np.max(np.abs(y))) if len(y) else 1.0
        return y / max(mx, 1e-6)
    except Exception:
        try:
            from moviepy.editor import VideoFileClip
            clip = VideoFileClip(video_path)
            snd = clip.audio.to_soundarray(fps=target_sr)
            if snd.ndim == 2: snd = snd.mean(axis=1)
            y = snd.astype(np.float32)
            mx = float(np.max(np.abs(y))) if len(y) else 1.0
            return y / max(mx, 1e-6)
        except Exception as e:
            raise RuntimeError(f"æ— æ³•ä»è§†é¢‘æå–éŸ³é¢‘ï¼Œè¯·æä¾› --audioã€‚åŸå› ï¼š{e}")

def load_video_for_model_and_render(video_path: str, target_size: Tuple[int,int]):
    """
    ç”¨ OpenCV è¯»è§†é¢‘ï¼š
      - frames_model:   [N, H, W, 3] float32ï¼Œå·²åš ImageNet å½’ä¸€åŒ–ï¼ˆç”¨äºæ¨¡å‹ï¼‰
      - frames_render:  [N, H, W, 3] uint8ï¼ŒBGRï¼ˆç”¨äºæ¸²æŸ“å åŠ ï¼‰
      - fps: float
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened(): raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘ï¼š{video_path}")
    fps = cap.get(cv2.CAP_PROP_FPS)
    frames_m, frames_r = [], []
    W, H = int(target_size[0]), int(target_size[1])
    while True:
        ok, bgr = cap.read()
        if not ok: break
        bgr_rs = cv2.resize(bgr, (W, H), interpolation=cv2.INTER_LINEAR)
        rgb = cv2.cvtColor(bgr_rs, cv2.COLOR_BGR2RGB).astype(np.float32)/255.0
        rgb = (rgb - IMAGENET_MEAN) / IMAGENET_STD
        frames_m.append(rgb)               # æ¨¡å‹è¾“å…¥ï¼ˆå½’ä¸€åŒ–ï¼‰
        frames_r.append(bgr_rs.copy())     # æ¸²æŸ“åº•å›¾ï¼ˆBGR uint8ï¼‰
    cap.release()
    if not frames_m: raise RuntimeError("è§†é¢‘æ— å¸§")
    return np.stack(frames_m, 0), np.stack(frames_r, 0), float(fps)

# ---------------- æ»‘çª—ä¸åˆ‡ç‰‡ ----------------

def make_windows(duration_s: float, win_s: float, hop_s: float) -> List[Tuple[float,float]]:
    t, out = 0.0, []
    while t + win_s <= duration_s + 1e-6:
        out.append((t, t + win_s)); t += hop_s
    return out

def slice_audio(y: np.ndarray, sr: int, start: float, end: float) -> np.ndarray:
    i0, i1 = int(round(start*sr)), int(round(end*sr))
    seg = y[i0:i1]
    if len(seg) == 0: seg = np.zeros(max(1, i1-i0), dtype=np.float32)
    mx = float(np.max(np.abs(seg))) if len(seg) else 1.0
    return (seg / max(mx, 1e-6)).astype(np.float32)

def slice_video(frames_norm: np.ndarray, fps: float, start: float, v_frames: int, v_stride: int) -> np.ndarray:
    n, h, w, c = frames_norm.shape
    start_idx = int(round(start * fps))
    idxs = [min(max(0, start_idx + i*v_stride), n-1) for i in range(v_frames)]
    clip = frames_norm[idxs]                 # [T,H,W,3]
    clip = np.transpose(clip, (0,3,1,2))     # [T,3,H,W]
    return clip.astype(np.float32)

# ---------------- æ¨ç†è¾…åŠ© ----------------

def logits_to_prob_defect(logits: torch.Tensor) -> np.ndarray:
    return F.softmax(logits, dim=-1)[:,1].detach().cpu().numpy()

def merge_binary_segments(times: List[Tuple[float,float]], preds: np.ndarray) -> List[Tuple[float,float,int]]:
    if len(times) == 0: return []
    out = []
    cur_s, cur_e, cur_y = times[0][0], times[0][1], int(preds[0])
    for (s,e), y in zip(times[1:], preds[1:]):
        y = int(y)
        if y == cur_y and abs(s - cur_e) <= 1e-6:
            cur_e = e
        else:
            out.append((cur_s, cur_e, cur_y)); cur_s, cur_e, cur_y = s, e, y
    out.append((cur_s, cur_e, cur_y))
    return out

# ---------------- éŸ³é¢‘å¯è§†åŒ–ï¼ˆæ¢…å°”è°±/æ³¢å½¢ï¼‰ ----------------

def make_melspec_image(y: np.ndarray, sr: int, width: int, height: int) -> np.ndarray:
    """
    ç”Ÿæˆå½©è‰²æ¢…å°”è°± BGR å›¾åƒï¼Œå¤§å° (height, width, 3) uint8ï¼›è‹¥ librosa ä¸å¯ç”¨ï¼Œå›é€€ä¸ºæ³¢å½¢å›¾ã€‚
    """
    try:
        import librosa
        S = librosa.feature.melspectrogram(y=y.astype(np.float32), sr=sr,
                                           n_fft=512, hop_length=160, win_length=400,
                                           n_mels=64, fmin=50, fmax=sr//2)
        S = librosa.power_to_db(S, ref=np.max)  # [n_mels, Tm]
        S = np.flipud(S)  # ä½é¢‘åœ¨ä¸‹
        S = (S - S.min()) / max(S.max() - S.min(), 1e-6)  # 0~1
        S = (S * 255.0).astype(np.uint8)
        S = cv2.resize(S, (width, height), interpolation=cv2.INTER_LINEAR)
        color = cv2.applyColorMap(S, cv2.COLORMAP_TURBO)  # BGR
        return color
    except Exception:
        # å›é€€ï¼šæ³¢å½¢å›¾
        img = np.zeros((height, width, 3), dtype=np.uint8)
        if len(y) > 1:
            # å–ç­‰è·é‡‡æ ·ç‚¹ç»˜æŠ˜çº¿
            xs = np.linspace(0, len(y)-1, width).astype(int)
            yy = y[xs]
            yy = (yy - yy.min()) / max(yy.max()-yy.min(), 1e-6)  # 0~1
            yy = (1.0 - yy) * (height-1)                         # ç¿»è½¬ y è½´
            pts = np.stack([np.arange(width), yy], axis=1).astype(np.int32)
            cv2.polylines(img, [pts], isClosed=False, color=(200, 200, 50), thickness=1)
        cv2.putText(img, "Waveform (librosa not found)", (10, height-10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (180,180,180), 1, cv2.LINE_AA)
        return img

# ---------------- æ¸²æŸ“å¸¦æ ‡ç­¾è§†é¢‘ ----------------

def render_video_with_overlays(frames_bgr: np.ndarray,
                               fps: float,
                               probs_def: np.ndarray,
                               preds_bin: np.ndarray,
                               windows: List[Tuple[float,float]],
                               audio: np.ndarray,
                               sr: int,
                               out_path: Path,
                               thr: float,
                               show_audio: bool = True):
    """
    åœ¨æ¯å¸§ä¸Šå åŠ ï¼š
      - æ–‡æœ¬æ ‡ç­¾ï¼ˆGOOD/DEFECTï¼‰+ èƒŒæ™¯å—
      - æ¦‚ç‡æ¡ï¼ˆå½“å‰ p(defect)ï¼‰
      - ï¼ˆå¯é€‰ï¼‰åº•éƒ¨éŸ³é¢‘å¯è§†åŒ–ï¼ˆæ¢…å°”è°±/æ³¢å½¢ï¼‰+ å½“å‰æ—¶é—´ç«–çº¿
    è¾“å‡ºï¼šMP4 è§†é¢‘ï¼ˆä¸è¾“å…¥å¸§åŒå°ºå¯¸ï¼‰
    """
    H, W = frames_bgr.shape[1], frames_bgr.shape[2]
    # é¢„è®¡ç®—ï¼šå¸§æ—¶é—´ -> æœ€è¿‘çª—ç´¢å¼•
    centers = np.array([(s+e)/2 for s,e in windows], dtype=float)
    num_frames = len(frames_bgr)
    times_f = np.arange(num_frames) / max(fps, 1e-6)
    idx = np.searchsorted(centers, times_f, side='left')
    idx = np.clip(idx, 0, len(centers)-1)
    # æ¯”è¾ƒç›¸é‚»ï¼Œé€‰æœ€è¿‘ä¸­å¿ƒ
    left = np.maximum(idx-1, 0)
    choose_left = (np.abs(centers[left] - times_f) < np.abs(centers[idx] - times_f))
    idx = np.where(choose_left, left, idx)
    prob_f = probs_def[idx]
    pred_f = preds_bin[idx]

    # éŸ³é¢‘å›¾ï¼ˆå®½ä¸å¸§ä¸€è‡´ï¼Œé«˜åº¦å›ºå®š 96 åƒç´ ï¼‰
    spec_h = 96
    spec_img = make_melspec_image(audio, sr, width=W, height=spec_h) if show_audio else None

    # Writer
    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out_path.parent.mkdir(parents=True, exist_ok=True)
    vw = cv2.VideoWriter(str(out_path), fourcc, fps, (W, H))
    if not vw.isOpened():
        raise RuntimeError("æ— æ³•åˆ›å»ºè§†é¢‘å†™å…¥å™¨ï¼Œç¡®è®¤ç¼–è§£ç å™¨æ˜¯å¦å¯ç”¨ï¼ˆå»ºè®®ä½¿ç”¨ mp4v / H264ï¼‰")

    # å åŠ æ ·å¼
    bar_h = 14            # æ¦‚ç‡æ¡é«˜åº¦
    pad = 6               # å†…è¾¹è·
    label_font = cv2.FONT_HERSHEY_SIMPLEX

    for i in range(num_frames):
        frame = frames_bgr[i].copy()

        # 1) æ¦‚ç‡æ¡ï¼ˆé¡¶éƒ¨ï¼‰
        p = float(prob_f[i])
        cv2.rectangle(frame, (pad, pad), (W-pad, pad+bar_h), (60,60,60), thickness=-1)
        fill = int((W-2*pad) * np.clip(p, 0, 1))
        cv2.rectangle(frame, (pad, pad), (pad+fill, pad+bar_h), (0,0,255), thickness=-1)  # çº¢è‰²è¡¨ç¤ºç¼ºé™·æ¦‚ç‡

        # 2) æ ‡ç­¾åº•æ¿ + æ–‡æœ¬
        label = "DEFECT" if pred_f[i]==1 else "GOOD"
        color = (40,40,220) if pred_f[i]==1 else (40,180,40)   # (B,G,R)
        text = f"{label}  P(def)={p:.2f}"
        (tw, th), bs = cv2.getTextSize(text, label_font, 0.7, 2)
        box_w, box_h = tw + 2*pad, th + 2*pad
        # æ”¾åœ¨å·¦ä¸Šè§’ã€æ¦‚ç‡æ¡ä¸‹æ–¹
        cv2.rectangle(frame, (pad, pad*2+bar_h), (pad+box_w, pad*2+bar_h+box_h), color, thickness=-1)
        cv2.putText(frame, text, (pad+pad, pad*2+bar_h+th+1), label_font, 0.7, (255,255,255), 2, cv2.LINE_AA)

        # 3) éŸ³é¢‘å¯è§†åŒ–ï¼ˆåº•éƒ¨è¦†ç›–ä¸€æ¡é«˜ spec_h çš„å¸¦çŠ¶åŒºåŸŸï¼‰+ å½“å‰æ—¶é—´çº¿
        if spec_img is not None:
            y0 = H - spec_h
            overlay = frame[y0:H, 0:W].copy()
            alpha = 0.55
            # æ··åˆæ¢…å°”è°±
            blended = cv2.addWeighted(spec_img, alpha, overlay, 1-alpha, 0)
            # å½“å‰æ—¶é—´çº¿ï¼ˆç™½è‰²ç«–çº¿ï¼‰
            x_now = int((i / max(num_frames-1, 1)) * (W-1))
            cv2.line(blended, (x_now, 0), (x_now, spec_h-1), (255,255,255), 1, cv2.LINE_AA)
            frame[y0:H, 0:W] = blended

            # é˜ˆå€¼æç¤º
            cv2.putText(frame, f"Audio Vis", (W-150, H-8), label_font, 0.5, (230,230,230), 1, cv2.LINE_AA)

        vw.write(frame)

    vw.release()
    print(f"ğŸ¬ å·²å†™å‡ºå¸¦æ ‡ç­¾è§†é¢‘ï¼š{out_path}")

# ---------------- æ¦‚ç‡æ—¶é—´è½´å›¾ ----------------

def draw_timeline(times: List[Tuple[float,float]], p_def: np.ndarray, thr: float,
                  merged: List[Tuple[float,float,int]], out_png: Path):
    xs = np.array([(s+e)/2 for s,e in times], dtype=float)
    plt.figure(figsize=(12,4))
    plt.plot(xs, p_def, linewidth=1.5, label="P(defect)")
    plt.axhline(thr, ls='--', c='r', lw=1, label=f"thr={thr:.2f}")
    for s,e,y in merged:
        if y==1:
            plt.axvspan(s, e, color='tomato', alpha=0.2)
        else:
            plt.axvspan(s, e, color='steelblue', alpha=0.1)
    plt.ylim([-0.05, 1.05]); plt.xlabel("Time (s)"); plt.ylabel("Probability")
    plt.title("Defect Probability Timeline"); plt.legend()
    out_png.parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout(); plt.savefig(out_png, dpi=150); plt.close()

# ---------------- æ¨¡å‹åŠ è½½ ----------------

def load_model(checkpoint: str, config: dict, device: torch.device) -> torch.nn.Module:
    import numpy as np
    print(f"[åŠ è½½æ¨¡å‹] {checkpoint}")
    model = EnhancedAVDetector(config)
    try:
        # å…ˆå°è¯•å®‰å…¨æ¨¡å¼ï¼ˆPyTorch 2.4+ï¼‰
        try:
            torch.serialization.add_safe_globals([np.core.multiarray.scalar])
        except Exception:
            pass
        ckpt = torch.load(checkpoint, map_location=device, weights_only=True)
        safe_mode = True
    except Exception as e:
        print(f"âš ï¸ weights_only=True å¤±è´¥ï¼ˆ{e.__class__.__name__}ï¼‰ï¼Œå›é€€åˆ°ä¼ ç»ŸåŠ è½½ã€‚")
        ckpt = torch.load(checkpoint, map_location=device)  # ä»…åœ¨ä½ ä¿¡ä»» ckpt æ—¶ä½¿ç”¨
        safe_mode = False
    state = ckpt.get('model_state_dict', ckpt)
    missing, unexpected = model.load_state_dict(state, strict=False)
    if missing or unexpected:
        print(f"â„¹ï¸ state_dict å¯¹é½ï¼šmissing={len(missing)}, unexpected={len(unexpected)}")
    print(f"âœ… æ¨¡å‹å·²åŠ è½½ï¼ˆ{'å®‰å…¨' if safe_mode else 'ä¼ ç»Ÿ'}æ¨¡å¼ï¼‰")
    return model.to(device).eval()

# ---------------- ä¸»æµç¨‹ ----------------

def main():
    ap = argparse.ArgumentParser("æ•´æ®µè§†é¢‘ç¼ºé™·æ£€æµ‹ï¼ˆæ»‘çª— + è§†é¢‘æ¸²æŸ“ï¼‰")
    ap.add_argument("--video", required=True, help="è§†é¢‘æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--audio", default=None, help="ï¼ˆå¯é€‰ï¼‰éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼›ä¸æä¾›åˆ™ä»è§†é¢‘æå–")
    ap.add_argument("--checkpoint", required=True)
    ap.add_argument("--config", required=True)
    ap.add_argument("--out_dir", default="outputs/infer")
    # æ»‘çª—/é‡‡æ ·
    ap.add_argument("--win_s", type=float, default=None, help="éŸ³é¢‘çª—å£ç§’ï¼›é»˜è®¤ä» config.data.max_audio_length")
    ap.add_argument("--hop_s", type=float, default=0.10)
    ap.add_argument("--v_frames", type=int, default=None, help="æ¯çª—è§†é¢‘å¸§æ•°ï¼›é»˜è®¤ä» config.data.max_video_frames")
    ap.add_argument("--v_stride", type=int, default=1)
    # åˆ¤å®šé˜ˆå€¼
    ap.add_argument("--thr", type=float, default=None, help="ç¼ºé™·æ¦‚ç‡é˜ˆå€¼ï¼›é»˜è®¤ 0.5 æˆ– metrics.yaml ä¸­ best_threshold")
    # æ¸²æŸ“å¼€å…³
    ap.add_argument("--render_video", action="store_true", help="å¯¼å‡ºå¸¦æ ‡ç­¾çš„æ¼”ç¤ºè§†é¢‘")
    ap.add_argument("--show_audio", action="store_true", help="åœ¨è§†é¢‘åº•éƒ¨å åŠ éŸ³é¢‘å¯è§†åŒ–ï¼ˆæ¢…å°”è°±/æ³¢å½¢ï¼‰")
    args = ap.parse_args()

    out_dir = Path(args.out_dir); out_dir.mkdir(parents=True, exist_ok=True)
    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    sr = int(cfg["data"].get("audio_sr", 16000))
    H, W = cfg["data"].get("video_size", [224,224])
    win_s    = float(args.win_s if args.win_s is not None else cfg["data"].get("max_audio_length", 0.3))
    v_frames = int(args.v_frames if args.v_frames is not None else cfg["data"].get("max_video_frames", 16))
    v_stride = int(args.v_stride)

    # é˜ˆå€¼ï¼šä¼˜å…ˆè¯»å–æœ€è¿‘è¯„ä¼°çš„ metrics.yamlï¼›å¦åˆ™ 0.5
    thr = args.thr
    if thr is None:
        metrics_yaml = Path(args.checkpoint).parent.parent / "evaluation" / "metrics.yaml"
        thr = 0.5
        if metrics_yaml.exists():
            try:
                m = yaml.safe_load(metrics_yaml.read_text("utf-8"))
                thr = float(m.get("best_threshold", 0.5))
            except Exception:
                pass

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = load_model(args.checkpoint, cfg, device)

    # è¯»éŸ³è§†é¢‘ï¼ˆæ¨¡å‹å¸§ + æ¸²æŸ“å¸§ï¼‰
    frames_model, frames_render, fps = load_video_for_model_and_render(args.video, (W, H))
    if args.audio and Path(args.audio).exists():
        audio = load_audio(args.audio, sr)
    else:
        audio = extract_audio_from_video(args.video, sr)

    dur_video = len(frames_model) / max(fps, 1e-6)
    dur_audio = len(audio) / float(sr)
    duration  = min(dur_video, dur_audio)
    windows   = make_windows(duration, win_s, args.hop_s)
    if not windows: raise RuntimeError("è§†é¢‘è¿‡çŸ­ï¼Œæœªç”Ÿæˆä»»ä½•çª—å£ã€‚")

    # æ¨ç†
    probs_def, preds_bin = [], []
    with torch.no_grad():
        for (s, e) in windows:
            a_seg = slice_audio(audio, sr, s, e)                        # [L]  â† æ³¢å½¢ (B,T) æœŸæœ›
            v_seg = slice_video(frames_model, fps, s, v_frames, v_stride)

            a_t = torch.from_numpy(a_seg).unsqueeze(0).to(device)       # [1, L] âœ…
            v_t = torch.from_numpy(v_seg).unsqueeze(0).to(device)       # [1, T, 3, H, W]

            try:
                out = model(v_t, a_t, return_aux=False)                 # ä¼˜å…ˆèµ°æ³¢å½¢
            except ValueError as err:
                if "Cannot determine input type" not in str(err):
                    raise
                # å›é€€ï¼šlog-melï¼ˆ64 binsï¼‰
                import librosa
                S = librosa.feature.melspectrogram(
                    y=a_seg.astype(np.float32), sr=sr,
                    n_fft=512, hop_length=160, win_length=400,
                    n_mels=64, fmin=50, fmax=sr//2
                )
                S = librosa.power_to_db(S, ref=np.max).T                # [Tm, 64]
                a_t = torch.from_numpy(S.astype(np.float32)).unsqueeze(0).to(device)  # [1, Tm, 64]
                out = model(v_t, a_t, return_aux=False)

            p = logits_to_prob_defect(out["clip_logits"])               # [1] -> ndarray
            probs_def.append(float(p[0]))
            preds_bin.append(int(p[0] >= thr))

    probs_def = np.asarray(probs_def, dtype=float)
    preds_bin = np.asarray(preds_bin, dtype=int)

    # åˆå¹¶è¿ç»­åŒºæ®µå¹¶å†™æŠ¥å‘Š/å›¾è¡¨ï¼ˆä¿æŒä½ çš„åŸæœ‰äº§ç‰©ï¼‰
    merged = merge_binary_segments(windows, preds_bin)

    seg_csv = out_dir / "segments.csv"
    with seg_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f); w.writerow(["start_s","end_s","prob_defect","pred"])
        for (s,e), p, y in zip(windows, probs_def, preds_bin):
            w.writerow([f"{s:.3f}", f"{e:.3f}", f"{p:.6f}", int(y)])

    merged_csv = out_dir / "merged_segments.csv"
    with merged_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f); w.writerow(["start_s","end_s","label"])
        for s,e,y in merged:
            w.writerow([f"{s:.3f}", f"{e:.3f}", "defect" if y==1 else "good"])

    # æ¦‚ç‡æ—¶é—´è½´
    timeline_png = out_dir / "timeline.png"
    draw_timeline(windows, probs_def, thr, merged, timeline_png)

    # è§†é¢‘çº§ç»“è®º
    total = sum(e-s for s,e in windows)
    defect_time = sum(e-s for s,e,y in merged if y==1)
    ratio = defect_time / max(total, 1e-9)
    video_label = "defect" if ratio > 0.5 else "good"
    with (out_dir/"summary.txt").open("w", encoding="utf-8") as f:
        f.write(f"video: {args.video}\n")
        f.write(f"duration_s: {duration:.3f}\n")
        f.write(f"windows: {len(windows)}, win_s={win_s}, hop_s={args.hop_s}, v_frames={v_frames}, v_stride={v_stride}\n")
        f.write(f"threshold: {thr:.3f}\n")
        f.write(f"defect_time_s: {defect_time:.3f} / {total:.3f} ({ratio*100:.2f}%)\n")
        f.write(f"video_label: {video_label}\n")

    print("âœ… æ¨ç†å®Œæˆï¼ˆCSV/å›¾è¡¨/æŠ¥å‘Š å·²å†™å‡ºï¼‰")
    print(f"  - {seg_csv}")
    print(f"  - {merged_csv}")
    print(f"  - {timeline_png}")
    print(f"  - {out_dir/'summary.txt'}")

    # æ¸²æŸ“å¸¦æ ‡ç­¾è§†é¢‘
    if args.render_video:
        out_mp4 = out_dir / "annotated.mp4"
        render_video_with_overlays(frames_render, fps, probs_def, preds_bin, windows,
                                   audio, sr, out_mp4, thr, show_audio=args.show_audio)

        print(f"ğŸ¯ è§†é¢‘çº§ç»“è®ºï¼š{video_label}ï¼ˆç¼ºé™·æ—¶é•¿å æ¯” {ratio*100:.2f}%ï¼‰")

if __name__ == "__main__":
    main()
