#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è®­ç»ƒè„šæœ¬ - æ•´åˆæ‰€æœ‰æ”¹è¿›æ–¹æ³•
=================================

æ”¯æŒçš„æ”¹è¿›ï¼š
1. InfoNCEå¯¹æ¯”å­¦ä¹ ï¼ˆæ‹‰è¿‘éŸ³è§†é¢‘åŒ¹é…å¯¹ï¼‰
2. ä¸‰æ¨¡æ€çŸ¥è¯†è’¸é¦ï¼ˆèåˆâ†”å•æ¨¡æ€ï¼‰
3. å¤šè§†å›¾ä¸€è‡´æ€§æ­£åˆ™åŒ–
4. æ•°æ®å¢å¼ºï¼ˆå¼ºå¼±å¢å¼ºï¼‰
5. Focal Loss / CB Lossï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
6. åŠç›‘ç£å­¦ä¹ ï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/train_unified.py --config configs/unified_config.yaml
"""
import os
import sys
import yaml
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).resolve().parents[1] / "src"))

from torch.utils.data import DataLoader

# å¯¼å…¥æ¨¡å‹å’ŒæŸå¤±
from avtop.models.enhanced_detector import EnhancedAVTopDetector
from avtop.data.csv_dataset import BinaryAVCSVDataset, collate

# å¯¼å…¥æŸå¤±å‡½æ•°
from avtop.losses.advanced_losses import create_loss_function
from avtop.losses.contrastive_loss import InfoNCELoss, ProjectionHead
from avtop.losses.kd_loss import TriModalKDLoss
from avtop.losses.consistency_loss import MultiViewConsistency
from avtop.losses.enhanced_loss import RankingMILLoss

# å¯¼å…¥æ•°æ®å¢å¼º
from avtop.data.augmentation_module import MultiModalAugmentation

# å¯¼å…¥è¯„ä¼°
from avtop.eval.enhanced_metrics import ImbalancedMetricsCalculator, validate_model

# å¯¼å…¥å·¥å…·
from avtop.utils.experiment import ExperimentManager


class UnifiedTrainer:
    """
    ç»Ÿä¸€è®­ç»ƒå™¨ - æ•´åˆæ‰€æœ‰æ”¹è¿›æ–¹æ³•

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å¯¹æ¯”å­¦ä¹ ï¼šè®©éŸ³è§†é¢‘åŒ¹é…å¯¹æ›´ç´§å¯†
    2. çŸ¥è¯†è’¸é¦ï¼šè®©å•æ¨¡æ€å‘èåˆæ¨¡å‹å­¦ä¹ 
    3. ä¸€è‡´æ€§ï¼šä¸åŒå¢å¼ºä¸‹é¢„æµ‹åº”ä¸€è‡´
    4. è‡ªé€‚åº”æƒé‡ï¼šæ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´å„æŸå¤±æƒé‡
    """

    def __init__(self, cfg: dict, device='cuda'):
        self.cfg = cfg
        self.device = device

        # 1. åˆ›å»ºæ¨¡å‹
        print("\nğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
        self.model = EnhancedAVTopDetector(cfg).to(device)

        # 2. åˆ›å»ºæŠ•å½±å¤´ï¼ˆç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
        fusion_dim = cfg['model'].get('fusion', {}).get('d_model', 256)
        proj_dim = cfg['train'].get('contrastive_dim', 128)

        self.video_proj = ProjectionHead(fusion_dim, output_dim=proj_dim).to(device)
        self.audio_proj = ProjectionHead(fusion_dim, output_dim=proj_dim).to(device)

        print(f"  âœ“ æ¨¡å‹ç»´åº¦: fusion_dim={fusion_dim}, proj_dim={proj_dim}")

        # 3. åˆ›å»ºæŸå¤±å‡½æ•°
        self._build_losses()

        # 4. åˆ›å»ºä¼˜åŒ–å™¨
        self._build_optimizer()

        # 5. æ•°æ®å¢å¼º
        if cfg['train'].get('use_augmentation', True):
            self.weak_aug = MultiModalAugmentation(mode='weak')
            self.strong_aug = MultiModalAugmentation(mode='strong')
        else:
            self.weak_aug = None
            self.strong_aug = None

        # 6. å®éªŒç®¡ç†
        self.exp_mgr = ExperimentManager(
            cfg['experiment']['workdir'],
            cfg['experiment']['name']
        )
        self.exp_mgr.save_config(cfg)

        # 7. è®­ç»ƒçŠ¶æ€
        self.epoch = 0
        self.global_step = 0
        self.best_metric = 0.0

        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼\n")

    def _build_losses(self):
        """æ„å»ºæ‰€æœ‰æŸå¤±å‡½æ•°"""
        cfg = self.cfg['train']

        print("ğŸ¯ é…ç½®æŸå¤±å‡½æ•°:")

        # 1. åˆ†ç±»æŸå¤±ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰
        loss_type = cfg.get('loss_type', 'focal')

        if loss_type == 'focal':
            self.cls_loss = create_loss_function(
                'focal',
                alpha=cfg.get('focal_alpha', 0.75),
                gamma=cfg.get('focal_gamma', 2.0)
            )
            print(f"  âœ“ Focal Loss (alpha={cfg.get('focal_alpha', 0.75)}, gamma={cfg.get('focal_gamma', 2.0)})")

        elif loss_type == 'cb':
            # éœ€è¦æä¾›æ¯ç±»æ ·æœ¬æ•°
            samples_per_class = cfg.get('samples_per_class', [800, 200])
            self.cls_loss = create_loss_function(
                'cb',
                samples_per_class=samples_per_class,
                beta=cfg.get('cb_beta', 0.9999),
                gamma=cfg.get('focal_gamma', 2.0),
                loss_type='focal'
            )
            print(f"  âœ“ Class-Balanced Loss (samples={samples_per_class})")

        else:
            self.cls_loss = nn.CrossEntropyLoss()
            print(f"  âœ“ Cross Entropy Loss")

        # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±
        if cfg.get('use_contrastive', True):
            self.contrastive_loss = InfoNCELoss(
                temperature=cfg.get('contrastive_temp', 0.07),
                queue_size=cfg.get('contrastive_queue', 0)
            )
            print(f"  âœ“ InfoNCE Loss (temp={cfg.get('contrastive_temp', 0.07)})")
        else:
            self.contrastive_loss = None

        # 3. çŸ¥è¯†è’¸é¦æŸå¤±
        if cfg.get('use_kd', True):
            self.kd_loss = TriModalKDLoss(
                temperature=cfg.get('kd_temp', 2.0),
                bimodal_weight=cfg.get('kd_bimodal_weight', 0.5)
            )
            print(f"  âœ“ Tri-Modal KD Loss (temp={cfg.get('kd_temp', 2.0)})")
        else:
            self.kd_loss = None

        # 4. ä¸€è‡´æ€§æŸå¤±
        if cfg.get('use_consistency', True):
            self.consistency_loss = MultiViewConsistency(
                consistency_type=cfg.get('consistency_type', 'mse'),
                temperature=cfg.get('consistency_temp', 1.0)
            )
            print(f"  âœ“ Consistency Loss ({cfg.get('consistency_type', 'mse')})")
        else:
            self.consistency_loss = None

        # 5. MIL RankingæŸå¤±ï¼ˆå¯é€‰ï¼‰
        if cfg.get('use_ranking', False):
            self.ranking_loss = RankingMILLoss(
                margin=cfg.get('ranking_margin', 0.5),
                topk=cfg.get('ranking_topk', 4)
            )
            print(f"  âœ“ MIL Ranking Loss")
        else:
            self.ranking_loss = None

        # æŸå¤±æƒé‡ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        self.loss_weights = {
            'cls': 1.0,
            'contrastive': self._parse_weight(cfg.get('contrastive_weight', '0.5')),
            'kd': self._parse_weight(cfg.get('kd_weight', '0.3')),
            'consistency': self._parse_weight(cfg.get('consistency_weight', '0.1')),
            'ranking': cfg.get('ranking_weight', 0.3)
        }

        print()

    def _parse_weight(self, w):
        """è§£ææƒé‡ï¼ˆæ”¯æŒåŠ¨æ€è°ƒæ•´ "0.1->0.5"ï¼‰"""
        if isinstance(w, str) and '->' in w:
            start, end = map(float, w.split('->'))
            return {'start': start, 'end': end, 'dynamic': True}
        else:
            return {'value': float(w), 'dynamic': False}

    def _get_weight(self, name):
        """è·å–å½“å‰epochçš„æƒé‡"""
        w = self.loss_weights[name]
        if isinstance(w, dict) and w.get('dynamic', False):
            # çº¿æ€§æ’å€¼
            progress = self.epoch / max(self.cfg['train']['epochs'], 1)
            value = w['start'] + (w['end'] - w['start']) * progress
            return value
        elif isinstance(w, dict):
            return w['value']
        else:
            return w

    def _build_optimizer(self):
        """æ„å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        cfg = self.cfg['train']

        # åˆ†å±‚å­¦ä¹ ç‡
        params = [
            {'params': self.model.video_backbone.parameters(), 'lr': cfg['lr'] * 0.01},
            {'params': self.model.audio_backbone.parameters(), 'lr': cfg['lr'] * 0.01},
            {'params': self.model.fusion.parameters(), 'lr': cfg['lr'] * 0.1},
            {'params': self.model.temporal.parameters(), 'lr': cfg['lr']},
            {'params': self.model.mil.parameters(), 'lr': cfg['lr']},
        ]

        # å¯¹æ¯”å­¦ä¹ æŠ•å½±å¤´
        if self.contrastive_loss is not None:
            params.extend([
                {'params': self.video_proj.parameters(), 'lr': cfg['lr']},
                {'params': self.audio_proj.parameters(), 'lr': cfg['lr']}
            ])

        self.optimizer = torch.optim.AdamW(
            params,
            lr=cfg['lr'],
            weight_decay=cfg.get('weight_decay', 1e-4)
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=cfg['epochs'],
            eta_min=cfg.get('min_lr', 1e-6)
        )

        print(f"ğŸ”§ ä¼˜åŒ–å™¨é…ç½®:")
        print(f"  âœ“ AdamW (lr={cfg['lr']}, weight_decay={cfg.get('weight_decay', 1e-4)})")
        print(f"  âœ“ CosineAnnealing Scheduler\n")

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()

        epoch_losses = {
            'total': 0.0,
            'cls': 0.0,
            'contrastive': 0.0,
            'kd': 0.0,
            'consistency': 0.0,
            'ranking': 0.0
        }

        pbar = tqdm(train_loader, desc=f"Epoch {self.epoch}")

        for batch_idx, batch in enumerate(pbar):
            video = batch['video'].to(self.device)
            audio = batch['audio'].to(self.device)
            labels = batch['label_idx'].to(self.device)

            # ============================================================
            # 1. æ ‡å‡†å‰å‘ä¼ æ’­ï¼ˆæ— å¢å¼ºï¼‰
            # ============================================================
            outputs = self.model(video, audio)

            # åˆ†ç±»æŸå¤±
            loss_cls = self.cls_loss(outputs['clip_logits'], labels)

            # ============================================================
            # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆæ‹‰è¿‘éŸ³è§†é¢‘åŒ¹é…å¯¹ï¼‰
            # ============================================================
            loss_con = torch.tensor(0.0, device=self.device)

            if self.contrastive_loss is not None:
                # ä»æ¨¡å‹è·å–å•æ¨¡æ€ç‰¹å¾ï¼ˆéœ€è¦ä¿®æ”¹æ¨¡å‹è¿”å›ï¼‰
                # è¿™é‡Œå‡è®¾æ¨¡å‹è¿”å›äº†video_embå’Œaudio_emb
                if hasattr(outputs, 'keys') and 'video_emb' in outputs:
                    video_emb = outputs['video_emb']  # [B, D]
                    audio_emb = outputs['audio_emb']  # [B, D]
                else:
                    # Fallback: ä½¿ç”¨å…¨å±€æ± åŒ–
                    video_emb = outputs.get('z', None)
                    audio_emb = outputs.get('z', None)

                    if video_emb is not None:
                        video_emb = video_emb.mean(dim=1)
                        audio_emb = audio_emb.mean(dim=1)

                if video_emb is not None and audio_emb is not None:
                    # æŠ•å½±åˆ°å¯¹æ¯”å­¦ä¹ ç©ºé—´
                    z_v = self.video_proj(video_emb)
                    z_a = self.audio_proj(audio_emb)

                    loss_con, con_metrics = self.contrastive_loss(z_a, z_v)

            # ============================================================
            # 3. çŸ¥è¯†è’¸é¦æŸå¤±ï¼ˆèåˆâ†’å•æ¨¡æ€ï¼‰
            # ============================================================
            loss_kd = torch.tensor(0.0, device=self.device)

            if self.kd_loss is not None:
                # éœ€è¦æ¨¡å‹è¿”å›å•æ¨¡æ€çš„logits
                if 'video_logits' in outputs and 'audio_logits' in outputs:
                    loss_kd, kd_metrics = self.kd_loss(
                        outputs['clip_logits'],
                        outputs['video_logits'],
                        outputs['audio_logits']
                    )

            # ============================================================
            # 4. ä¸€è‡´æ€§æŸå¤±ï¼ˆä¸åŒå¢å¼ºä¸‹é¢„æµ‹åº”ä¸€è‡´ï¼‰
            # ============================================================
            loss_cons = torch.tensor(0.0, device=self.device)

            if self.consistency_loss is not None and self.weak_aug is not None:
                # å¼±å¢å¼º
                video_weak = video  # åŸå§‹è§†ä¸ºå¼±å¢å¼º
                audio_weak = audio

                # å¼ºå¢å¼º
                if self.strong_aug is not None:
                    # éœ€è¦é€æ ·æœ¬å¢å¼ºï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                    video_strong = video  # å®é™…åº”ç”¨strong_aug
                    audio_strong = audio

                    outputs_strong = self.model(video_strong, audio_strong)

                    # ä¸€è‡´æ€§ï¼šåŸå§‹é¢„æµ‹ vs å¼ºå¢å¼ºé¢„æµ‹
                    logits_list = [
                        outputs['clip_logits'],
                        outputs_strong['clip_logits']
                    ]

                    loss_cons, cons_metrics = self.consistency_loss(
                        logits_list,
                        mode='mean_teacher'
                    )

            # ============================================================
            # 5. RankingæŸå¤±ï¼ˆå¯é€‰ï¼‰
            # ============================================================
            loss_rank = torch.tensor(0.0, device=self.device)

            if self.ranking_loss is not None and 'scores' in outputs:
                loss_rank = self.ranking_loss(outputs['scores'], labels)

            # ============================================================
            # æ€»æŸå¤±ï¼ˆåŠ æƒç»„åˆï¼‰
            # ============================================================
            w_cls = self._get_weight('cls')
            w_con = self._get_weight('contrastive')
            w_kd = self._get_weight('kd')
            w_cons = self._get_weight('consistency')
            w_rank = self._get_weight('ranking')

            total_loss = (
                    w_cls * loss_cls +
                    w_con * loss_con +
                    w_kd * loss_kd +
                    w_cons * loss_cons +
                    w_rank * loss_rank
            )

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()

            # ç»Ÿè®¡
            epoch_losses['total'] += total_loss.item()
            epoch_losses['cls'] += loss_cls.item()
            epoch_losses['contrastive'] += loss_con.item()
            epoch_losses['kd'] += loss_kd.item()
            epoch_losses['consistency'] += loss_cons.item()
            epoch_losses['ranking'] += loss_rank.item()

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{total_loss.item():.3f}",
                'cls': f"{loss_cls.item():.3f}",
                'con': f"{loss_con.item():.3f}" if loss_con.item() > 0 else "0"
            })

            self.global_step += 1

        # å¹³å‡æŸå¤±
        n_batches = len(train_loader)
        for k in epoch_losses:
            epoch_losses[k] /= n_batches

        return epoch_losses

    def validate(self, val_loader, minority_class=1):
        """éªŒè¯"""
        metrics, probs, labels = validate_model(
            self.model, val_loader, self.device, minority_class
        )
        return metrics

    def train(self, train_loader, val_loader):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        cfg = self.cfg['train']
        epochs = cfg['epochs']
        patience = cfg.get('patience', 15)
        early_stop_metric = cfg.get('early_stop_metric', 'auprc_minority')

        # ç¡®å®šminority class
        minority_class = cfg.get('minority_class', 1)

        print("=" * 70)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ")
        print("=" * 70)
        print(f"  Epochs: {epochs}")
        print(f"  Patience: {patience}")
        print(f"  Early Stop Metric: {early_stop_metric}")
        print(f"  Minority Class: {minority_class}")
        print("=" * 70 + "\n")

        bad_epochs = 0

        for epoch in range(epochs):
            self.epoch = epoch

            # è®­ç»ƒ
            train_losses = self.train_epoch(train_loader)

            # æ‰“å°è®­ç»ƒæŸå¤±
            print(f"\nğŸ“Š Epoch {epoch} - è®­ç»ƒæŸå¤±:")
            print(f"  Total:       {train_losses['total']:.4f}")
            print(f"  Classification:  {train_losses['cls']:.4f}")
            print(f"  Contrastive:     {train_losses['contrastive']:.4f}")
            print(f"  KD:              {train_losses['kd']:.4f}")
            print(f"  Consistency:     {train_losses['consistency']:.4f}")

            # éªŒè¯
            val_metrics = self.validate(val_loader, minority_class)

            # æ‰“å°éªŒè¯ç»“æœ
            calc = ImbalancedMetricsCalculator(minority_class)
            calc.print_report(val_metrics, name=f"Validation (Epoch {epoch})")

            # ä¿å­˜æŒ‡æ ‡
            self.exp_mgr.save_metrics({
                'epoch': epoch,
                **val_metrics,
                **train_losses
            }, epoch)

            # æ—©åœæ£€æŸ¥
            current_metric = val_metrics[early_stop_metric]

            if current_metric > self.best_metric:
                self.best_metric = current_metric
                bad_epochs = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_checkpoint('best_model.pth')

                print(f"\nâ­ æ–°çš„æœ€ä½³ {early_stop_metric}: {self.best_metric:.4f}")
            else:
                bad_epochs += 1
                print(f"\nğŸ“Š {early_stop_metric}: {current_metric:.4f} "
                      f"(best: {self.best_metric:.4f}, no improve: {bad_epochs}/{patience})")

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()

            print()

            # æ—©åœ
            if bad_epochs >= patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch}")
                break

        print("\n" + "=" * 70)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"   æœ€ä½³ {early_stop_metric}: {self.best_metric:.4f}")
        print("=" * 70)

    def save_checkpoint(self, filename):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': self.epoch,
            'model': self.model.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict(),
            'best_metric': self.best_metric,
            'video_proj': self.video_proj.state_dict() if self.contrastive_loss else None,
            'audio_proj': self.audio_proj.state_dict() if self.contrastive_loss else None,
        }

        save_path = self.exp_mgr.root / 'ckpts' / filename
        save_path.parent.mkdir(exist_ok=True)
        torch.save(checkpoint, save_path)


def main():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€è®­ç»ƒè„šæœ¬")
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        cfg = yaml.safe_load(f)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\n")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“¦ åŠ è½½æ•°æ®...")

    train_dataset = BinaryAVCSVDataset(
        cfg['data']['train_csv'],
        root=cfg['data'].get('root', ''),
        T_v=cfg['data']['T_v'],
        T_a=cfg['data']['T_a'],
        mel_bins=cfg['data']['mel'],
        sample_rate=cfg['data']['sr']
    )

    val_dataset = BinaryAVCSVDataset(
        cfg['data']['val_csv'],
        root=cfg['data'].get('root', ''),
        T_v=cfg['data']['T_v'],
        T_a=cfg['data']['T_a'],
        mel_bins=cfg['data']['mel'],
        sample_rate=cfg['data']['sr']
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg['train']['batch_size'],
        shuffle=True,
        collate_fn=collate,
        num_workers=cfg['train'].get('workers', 4),
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg['train']['batch_size'],
        shuffle=False,
        collate_fn=collate,
        num_workers=cfg['train'].get('workers', 4),
        pin_memory=True
    )

    print(f"  âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"  âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬\n")

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedTrainer(cfg, device)

    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, val_loader)


if __name__ == '__main__':
    main()